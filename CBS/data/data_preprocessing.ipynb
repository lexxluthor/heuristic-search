{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBS for MAPF\n",
    "<center><img src=\"img/1.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала надо превратить все тесты в формат .xml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dicttoxml in /home/lexx/anaconda3/envs/pythonds/lib/python3.11/site-packages (1.7.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install dicttoxml\n",
    "import os\n",
    "from dataclasses import dataclass, asdict, field\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from dicttoxml import dicttoxml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Map:\n",
    "    map_file_path: str\n",
    "    grid: list[list[str]] = field(init=False, default_factory=list)\n",
    "    width: int = field(init=False)\n",
    "    height: int = field(init=False)\n",
    "    name: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        with open(self.map_file_path) as f:\n",
    "            f.readline() # skipping map type\n",
    "            _, height = f.readline().split()\n",
    "            self.height = int(height)\n",
    "            _, width = f.readline().split()\n",
    "            self.width = int(width)\n",
    "            f.readline() # skipping map name\n",
    "\n",
    "            for line in f.readlines():\n",
    "                self.grid.append([ str(int(s in ('@', 'T'))) for s in line ])\n",
    "        self.name = self.map_file_path.split('/')[-1].replace('.', '_')\n",
    "\n",
    "    @property\n",
    "    def grid_as_str(self) -> list[str]:\n",
    "        return [' '.join(self.grid[h]) for h in range(self.height)]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scenario:\n",
    "    scen_file_path: str = field(repr=False)\n",
    "    tasks_count: int = 1\n",
    "    maxtime: int = 1000\n",
    "    single_execution: bool = False\n",
    "    starts: list[tuple[int, int]] = field(init=False, default_factory=list)\n",
    "    goals: list[tuple[int, int]] = field(init=False, default_factory=list)\n",
    "    agents_count: int = field(init=False, default=0)\n",
    "    starting_agent_number: int = 1\n",
    "    log_filename: str = None\n",
    "    log_filepath: str = 'log'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        with open(self.scen_file_path) as f:\n",
    "            f.readline() # skipping task version\n",
    "\n",
    "            for line in f.readlines():\n",
    "                _, _, _, _, start_i, start_j, goal_i, goal_j, _ = line.split()\n",
    "                self.starts.append((start_i, start_j))\n",
    "                self.goals.append((goal_i, goal_j))\n",
    "                \n",
    "            self.agents_count = len(self.starts)\n",
    "\n",
    "    @property\n",
    "    def options(self):\n",
    "        return {\n",
    "            'tasks_count': str(self.tasks_count),\n",
    "            'maxtime': str(self.maxtime),\n",
    "            'single_execution': str(self.single_execution).lower()\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Algorithm:\n",
    "    planner: str = 'cbs'\n",
    "    low_level: str = 'astar'\n",
    "    with_cat: bool = True\n",
    "    with_perfect_h: bool = False\n",
    "    with_card_conf: bool = True\n",
    "    with_disjoint_splitting: bool = False\n",
    "    focal_w: float = 1.0\n",
    "\n",
    "    def todict(self):\n",
    "        return {k: str(v).lower() for k, v in asdict(self).items()}\n",
    "        \n",
    "\n",
    "@dataclass \n",
    "class Config:\n",
    "    map: Map\n",
    "    algorithm: Algorithm\n",
    "    scenario: Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_xml(map: Map, algorithm: Algorithm, scen: Scenario, map_filename: str, agents_filename):\n",
    "    root = ET.Element('root')\n",
    "\n",
    "    # grid-related stuff\n",
    "    map_ = ET.SubElement(root, 'map')\n",
    "    \n",
    "    grid = ET.SubElement(map_, 'grid')\n",
    "    grid.attrib = {'width': str(map.width), 'height': str(map.height)}\n",
    "\n",
    "    for h in range(map.height):\n",
    "        row = ET.SubElement(grid, 'row')\n",
    "        row.text = map.grid_as_str[h]\n",
    "\n",
    "    # algorithm-related stuff\n",
    "    algorithm = dicttoxml(algorithm.todict(), custom_root='algorithm', \n",
    "                          xml_declaration=False, attr_type=False)\n",
    "    algorithm = ET.fromstring(algorithm)\n",
    "    root.append(algorithm)\n",
    "\n",
    "\n",
    "    # agents-related stuff\n",
    "    options = dicttoxml(scen.options, custom_root='options', \n",
    "                          xml_declaration=False, attr_type=False)\n",
    "    options = ET.fromstring(options)\n",
    "    agents_file = ET.SubElement(options, 'agents_file')\n",
    "    agents_file.text = agents_filename.split('.')[0]\n",
    "    agents_range = ET.SubElement(options, 'agents_range')\n",
    "    agents_range.attrib = {'min': str(scen.starting_agent_number), 'max': str(scen.agents_count)}\n",
    "    log_file = ET.SubElement(options, 'logfilename')\n",
    "    if scen.log_filename:\n",
    "        log_file.text = scen.log_filename\n",
    "        log_path = ET.SubElement(options, 'logpath')\n",
    "        log_path.text = scen.log_filepath\n",
    "    aggr_results = ET.SubElement(options, 'aggregated_results')\n",
    "    aggr_results.text = 'true'\n",
    "\n",
    "    root.append(options)\n",
    "\n",
    "    \n",
    "    # building tree and writing into file\n",
    "    tree = ET.ElementTree(root)\n",
    "\n",
    "    with open(map_filename, 'wb') as file:\n",
    "        ET.indent(tree, space='\\t', level=0)\n",
    "        tree.write(file, xml_declaration=True)\n",
    "\n",
    "def scenario_to_xml(scen: Scenario, filename: str):\n",
    "    root = ET.Element('root')\n",
    "\n",
    "    for agent_id in range(scen.agents_count):\n",
    "        start_i, start_j = scen.starts[agent_id]\n",
    "        goal_i, goal_j = scen.goals[agent_id] \n",
    "        agent = ET.SubElement(root, 'agent')\n",
    "        agent.attrib = {'id': str(agent_id),\n",
    "                      'start_i': start_j,\n",
    "                      'start_j': start_i,\n",
    "                      'goal_i': goal_j,\n",
    "                      'goal_j': goal_i,\n",
    "                      }\n",
    "\n",
    "    tree = ET.ElementTree(root)\n",
    "\n",
    "    filename = filename.split('.')[0] + f'-{scen.tasks_count}.xml'\n",
    "    with open(filename, 'wb') as file:\n",
    "        ET.indent(tree, space='\\t', level=0)\n",
    "        tree.write(file, xml_declaration=True)\n",
    "\n",
    "def movingai_task_to_xml(config: Config, map_filename: str, scen_filename: str):\n",
    "    scenario_to_xml(config.scenario, scen_filename)\n",
    "    map_to_xml(config.map, config.algorithm, config.scenario, map_filename, scen_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_results_to_pd(xml_filename: str) -> pd.DataFrame:\n",
    "    tree = ET.parse(xml_filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    agents_count = []\n",
    "    success_count = []\n",
    "    time = []\n",
    "    flowtime = []\n",
    "    makespan = []\n",
    "    sic = []\n",
    "\n",
    "    # Extracting data from XML\n",
    "    for result in root.findall('.//result'):\n",
    "        agents_count.append(int(result.attrib['agents_count']))\n",
    "        success_count.append(int(result.attrib['success_count']) == 1)\n",
    "        time.append(float(result.find('.//iteration').attrib['time']))\n",
    "        flowtime.append(int(result.find('.//iteration').attrib['flowtime']))\n",
    "        makespan.append(int(result.find('.//iteration').attrib['makespan']))\n",
    "        sic.append(int(result.find('.//iteration').attrib['makespan']))\n",
    "\n",
    "    return pd.DataFrame({'agents_count': agents_count, \n",
    "                         'success_count': success_count, \n",
    "                         'time': time,\n",
    "                         'flowtime': flowtime,\n",
    "                         'makespan': makespan,\n",
    "                         'SIC': sic}).set_index('agents_count')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty-48-48_map 48 48 2304 0.0\n",
      "Paris_1_256_map 256 256 65536 0.2791748046875\n",
      "room-64-64-16_map 64 64 4096 0.10986328125\n",
      "warehouse-20-40-10-2-2_map 340 164 55760 0.30494978479196555\n"
     ]
    }
   ],
   "source": [
    "for m in ['empty-48-48.map', 'Paris_1_256.map', 'room-64-64-16.map', 'warehouse-20-40-10-2-2.map']:\n",
    "    map_g = Map(m)\n",
    "    print(map_g.name, map_g.width, map_g.height, sum([sum(map(int, l)) for l in map_g.grid])/( map_g.width * map_g.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Becnhmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_PATHS = ['empty-48-48.map', 'Paris_1_256.map', 'room-64-64-16.map', 'warehouse-20-40-10-2-2.map']\n",
    "SCEN_PATHS = ['scen-empty/empty-48-48-even-2.scen', 'scen-paris/Paris_1_256-even-3.scen',\n",
    "              'scen-room/room-64-64-16-even-9.scen', 'scen-warehouse/warehouse-20-40-10-2-2-even-11.scen']\n",
    "\n",
    "@dataclass\n",
    "class AlgorithmSpecification:\n",
    "    name: str\n",
    "    params: dict\n",
    "\n",
    "CBS_ALGORITHM = AlgorithmSpecification('cbs', {'planner': 'cbs', 'with_perfect_h': True})\n",
    "CBS_DS_ALGORITHM = AlgorithmSpecification('cbs_ds', {'planner': 'cbs', 'with_perfect_h': True, 'with_disjoint_splitting': True})\n",
    "ECBS_30_ALGORITHM = AlgorithmSpecification('ecbs_30', {'planner': 'ecbs', 'with_perfect_h': True, 'low_level': 'focal_search', 'focal_w': 3.0})\n",
    "ECBS_179_ALGORITHM = AlgorithmSpecification('ecbs_179', {'planner': 'ecbs', 'with_perfect_h': True, 'low_level': 'focal_search', 'focal_w': 1.79})\n",
    "ALG_PARAMS = [CBS_ALGORITHM, CBS_DS_ALGORITHM, ECBS_30_ALGORITHM, ECBS_179_ALGORITHM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_result(alg: AlgorithmSpecification, map: Map, scen: Scenario) -> pd.DataFrame:\n",
    "    alg_name, alg_params = alg.name, alg.params\n",
    "    scen.agents_count = min(scen.agents_count, 300)\n",
    "    conf = Config(map, Algorithm(**alg_params), scen)\n",
    "    movingai_task_to_xml(conf, 'testmap.xml', 'testmap-agents.xml')\n",
    "    status = os.system('../source/CBS testmap.xml >/dev/null 2>&1')\n",
    "    if not status:\n",
    "        return xml_results_to_pd('testmap_log.xml').add_suffix('_' + alg_name)\n",
    "    else:\n",
    "        raise RuntimeError('C++ code failed with error.')\n",
    "\n",
    "def benchmarking(map_paths: list[str], \n",
    "                 scen_paths: list[str], \n",
    "                 algorithms_params: list[AlgorithmSpecification],\n",
    "                 max_time: int = 1000) -> list[tuple[str, pd.DataFrame]]:\n",
    "    results = []\n",
    "\n",
    "    for map, scen in zip(map_paths, scen_paths):\n",
    "        df_map = pd.DataFrame(columns=[\"agents_count\"]).set_index(\"agents_count\")\n",
    "        cur_map = Map(map_file_path=map)\n",
    "        cur_scen = Scenario(scen_file_path=scen, \n",
    "                maxtime=max_time)\n",
    "        for alg in tqdm(algorithms_params, desc=f\"Running algorithms on {cur_map.map_file_path}\"):\n",
    "            df_map = pd.concat([df_map, get_map_result(alg, cur_map, cur_scen)], axis=1, join='outer')\n",
    "        \n",
    "        results.append((cur_map.name, df_map))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running algorithms on data/empty-48-48.map: 100%|██████████| 4/4 [15:03<00:00, 225.84s/it]\n",
      "Running algorithms on data/Paris_1_256.map: 100%|██████████| 4/4 [2:23:30<00:00, 2152.51s/it]\n",
      "Running algorithms on data/room-64-64-16.map: 100%|██████████| 4/4 [1:17:51<00:00, 1167.84s/it]\n",
      "Running algorithms on data/warehouse-20-40-10-2-2.map: 100%|██████████| 4/4 [4:47:23<00:00, 4310.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "res = benchmarking(MAP_PATHS, SCEN_PATHS, ALG_PARAMS, max_time=60*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, r in res:\n",
    "    r.to_csv(name + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные готовы, теперь перейдем к их изучению!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
